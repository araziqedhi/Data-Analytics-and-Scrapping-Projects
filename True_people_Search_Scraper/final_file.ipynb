{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b34610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (e:\\anaconda folder\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Zenrows in e:\\anaconda folder\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda folder\\lib\\site-packages (from Zenrows) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda folder\\lib\\site-packages (from requests->Zenrows) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda folder\\lib\\site-packages (from requests->Zenrows) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda folder\\lib\\site-packages (from requests->Zenrows) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda folder\\lib\\site-packages (from requests->Zenrows) (3.3)\n",
      "Extracted Data for URL 0:\n",
      "                        Name                                Link\n",
      "0           Kasey M Chambers   /find/person/p2nl490268uuu9r6u08n\n",
      "1         Gregory L Pleasant   /find/person/p660rr0r48ul494l8rl6\n",
      "2         Lonnie A Carpenter  /find/person/pxluu4ulr840nllu00rr2\n",
      "3           Thomas E Shannon   /find/person/p209r28rlu2l86846r86\n",
      "4          Demarcus Pleasant   /find/person/pn026206642r4rnl84u2\n",
      "5          Bernice S Simpson  /find/person/px8r2l2ruln922690n202\n",
      "6                 Owen Carty  /find/person/px22264uun2l42llru8u2\n",
      "7  Magi Carpenter Carpenters  /find/person/pxu9nru88rnru462r8r82\n",
      "8             Dennis C Scott    /find/person/p48nnn4r242n84l2r2l\n",
      "9         Gregory L Pleasant    /find/person/pu298nn8292u0uln8l9\n",
      "Matched Records for URL 0:\n",
      "                 Name                               Link\n",
      "1  Gregory L Pleasant  /find/person/p660rr0r48ul494l8rl6\n",
      "9  Gregory L Pleasant   /find/person/pu298nn8292u0uln8l9\n",
      "Extracted Data for URL 1:\n",
      "                       Name                                Link\n",
      "0         Juliette Gaslonde   /find/person/p86r2l6420nn86u8200u\n",
      "1      Dionellys C Sandoval  /find/person/px44ur0442r0ur4088984\n",
      "2           Karla L Pentzke   /find/person/px9284nu9029246r0uu0\n",
      "3          Raquel M Morales   /find/person/p8nu696022r400nru8r6\n",
      "4             Yadira Pernas   /find/person/p69rrll2nl9u2r96r20l\n",
      "5     E Angulo Chang Hermes   /find/person/p8u806urr82nn0u8040r\n",
      "6  A Angulo Sandoval Hermes    /find/person/p8802204lrnlun86nn4\n",
      "7             Karla Pentzke   /find/person/pr0u2u09l492lllr249l\n",
      "8              Nelson Lemus   /find/person/pxn28u2nl6889r20409r\n",
      "9           Daniel Sandoval   /find/person/pnr8n4880l8n4ur9lr46\n",
      "Matched Records for URL 1:\n",
      "              Name                               Link\n",
      "9  Daniel Sandoval  /find/person/pnr8n4880l8n4ur9lr46\n",
      "Extracted Data for URL 2:\n",
      "                     Name                                Link\n",
      "0             Daniel Amey   /find/person/p688nl80099r06l06r04\n",
      "1      Janice M Eberhardt  /find/person/px8924lu9900682l49r6r\n",
      "2              Linda Wood   /find/person/pxu0lu62n2u8299n2nr0\n",
      "3          Gary K Simmons   /find/person/p4rrn9r04l20ruu26n42\n",
      "4         Leo C Davenport   /find/person/px9n0l8464ln2nr8r894\n",
      "5  Kathleen A Littlefield   /find/person/pn9l6u0822n02nl22862\n",
      "6         Nancy L Scherer   /find/person/plnln4l6094l04u2649n\n",
      "7        Joan T Eberhardt   /find/person/pn6r2n4840u00942l99l\n",
      "8        Kathryn C Hilton   /find/person/plul96rl8929unu86849\n",
      "9        Bradley J Berman  /find/person/px2l0l84l0n862n9lr229\n",
      "Matched Records for URL 2:\n",
      "Empty DataFrame\n",
      "Columns: [Name, Link]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17356\\1977592540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Perform the request to fetch the HTML content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Check for any HTTP errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\zenrows\\client.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, params, headers, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     ) -> requests.Response:\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_worker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     async def get_async(\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\zenrows\\client.py\u001b[0m in \u001b[0;36m_worker\u001b[1;34m(self, method, url, params, headers, data, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mfinal_headers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         return self.requests_session.request(\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_headers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         )\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda folder\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install Zenrows\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from zenrows import ZenRowsClient\n",
    "import re\n",
    "\n",
    "# Read the DataFrame from the provided Excel file\n",
    "df = pd.read_excel(\"./Project EXCEL FAST PEOPLE SEARCH (2) (1) (1).xlsx\")\n",
    "\n",
    "# Create an empty DataFrame to store the URLs\n",
    "url_df = pd.DataFrame(columns=['URL'])\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the address components from the DataFrame\n",
    "    street_address =str(row['Mailing Address'])\n",
    "    city = str(row['Mailing City'])\n",
    "    state = str(row['Mailing State'])\n",
    "    \n",
    "    # Format the address components for the URL\n",
    "    formatted_street_address = urllib.parse.quote(street_address)\n",
    "    formatted_citystatezip = urllib.parse.quote(f\"{city},{state}\")\n",
    "    \n",
    "    # Construct the URL\n",
    "    url = f\"https://www.truepeoplesearch.com/resultaddress?streetaddress={formatted_street_address}&citystatezip={formatted_citystatezip}\"\n",
    "    \n",
    "    # Create a temporary DataFrame for the URL\n",
    "    temp_df = pd.DataFrame({'URL': [url]})\n",
    "    \n",
    "    # Concatenate the temporary DataFrame with the main DataFrame\n",
    "    url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "url_df.to_csv(\"urls.csv\", index=False)\n",
    "\n",
    "# Create a ZenRowsClient instance\n",
    "client = ZenRowsClient(\"840911f2101dafeeb3b4c3f882bd083d4673a02e\")\n",
    "\n",
    "# Create an empty list to store the matched records\n",
    "matched_records = []\n",
    "\n",
    "# Iterate over the URLs in 'url_df' and compare with the corresponding entries in the 'short_data (1) (2)' DataFrame\n",
    "for index, row in url_df.iterrows():\n",
    "    url = row['URL']\n",
    "    params = {\"premium_proxy\": \"true\"}\n",
    "    \n",
    "    try:\n",
    "        # Perform the request to fetch the HTML content\n",
    "        response = client.get(url, params=params)\n",
    "        response.raise_for_status()  # Check for any HTTP errors\n",
    "        \n",
    "        # Assume `html` contains the HTML content of the page\n",
    "        html = response.text\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find all <div> elements with class=\"col-md-8\"\n",
    "        div_col_md_8_list = soup.find_all('div', class_='col-md-8')\n",
    "        \n",
    "        # Extract the text from each <div> element with class=\"h4\" inside the <div class=\"col-md-8\">\n",
    "        h4_texts = []\n",
    "        for div_col_md_8 in div_col_md_8_list:\n",
    "            h4_div = div_col_md_8.find('div', class_='h4')\n",
    "            if h4_div is not None:\n",
    "                h4_text = h4_div.get_text().strip()\n",
    "                h4_texts.append(h4_text)\n",
    "        \n",
    "        # Find all elements with the data-detail-link attribute\n",
    "        elements = soup.find_all(attrs={'data-detail-link': True})\n",
    "        \n",
    "        # Extract the href attribute from each element\n",
    "        links = []\n",
    "        for element in elements:\n",
    "            href_link = element.get('data-detail-link')\n",
    "            links.append(href_link)\n",
    "        \n",
    "        # Create a dictionary with the extracted data\n",
    "        data = {\n",
    "            \"Name\": h4_texts,\n",
    "            \"Link\": links\n",
    "        }\n",
    "        \n",
    "        # Create the DataFrame for the extracted data\n",
    "        df_extracted = pd.DataFrame(data)\n",
    "        \n",
    "        # Get the corresponding entry from the 'short_data (1) (2)' DataFrame\n",
    "        df_short_data_entry = df.loc[index]\n",
    "\n",
    "        # Get the first name from the 'short_data (1) (2)' DataFrame entry and strip whitespaces\n",
    "        owner1_first_name = df_short_data_entry['Owner 1 First Name']\n",
    "        if not isinstance(owner1_first_name, float):\n",
    "            owner1_first_name = owner1_first_name.strip()\n",
    "        else:\n",
    "            owner1_first_name = \"Not found\"\n",
    "        \n",
    "        owner2_first_name = df_short_data_entry['Owner 2 First Name']\n",
    "        if not isinstance(owner2_first_name, float):\n",
    "            owner2_first_name = owner2_first_name.strip()\n",
    "        else:\n",
    "            owner2_first_name = \"Not found\"\n",
    "\n",
    "        # Filter the extracted DataFrame to match the first name\n",
    "        if owner1_first_name and not isinstance(owner1_first_name, float):\n",
    "            # Filter using regular expression to match the first name\n",
    "            regex = re.compile(r\"\\b\" + re.escape(owner1_first_name.lower()) + r\"\\b\")\n",
    "            df_matched_records = df_extracted[df_extracted['Name'].str.lower().str.contains(regex)]\n",
    "        elif owner2_first_name:\n",
    "            # Filter using regular expression to match the first name\n",
    "            regex = re.compile(r\"\\b\" + re.escape(owner2_first_name.lower()) + r\"\\b\")\n",
    "            df_matched_records = df_extracted[df_extracted['Name'].str.lower().str.contains(regex)]\n",
    "        else:\n",
    "            print(\"Not found\")\n",
    "            df_matched_records = pd.DataFrame(columns=[\"Name\", \"Link\"])  # Create an empty DataFrame\n",
    "\n",
    "        # Print the matched records to check\n",
    "        print(f\"Extracted Data for URL {index}:\\n{df_extracted}\")\n",
    "        print(f\"Matched Records for URL {index}:\\n{df_matched_records}\")\n",
    "\n",
    "        # Append the matched records to the main list\n",
    "        matched_records.append(df_matched_records)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If there's an error with the current URL, handle it here\n",
    "        # You can print an error message or log the error for further analysis\n",
    "        print(f\"Error occurred for URL {index}: {e}\")\n",
    "        # You can choose to skip this URL and continue with the next one, or take any other appropriate action\n",
    "\n",
    "# Concatenate all matched records into the final DataFrame\n",
    "df_final = pd.concat(matched_records, ignore_index=True)\n",
    "\n",
    "# Print the matched records and the final DataFrame\n",
    "print(\"Matched Records:\")\n",
    "print(df_final)\n",
    "\n",
    "df_final.to_csv(\"./final_links.csv\")\n",
    "\n",
    "# Rest of the code...\n",
    "# (Continue with the rest of the program as before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'final_url' DataFrame from the CSV file and set the index to be the first column\n",
    "final_url = pd.read_csv(\"./final_links.csv\", index_col=0)\n",
    "\n",
    "# Drop the 'Name' column from the DataFrame\n",
    "final_url.drop(\"Name\", axis=1, inplace=True)\n",
    "\n",
    "# Add \"https://www.truepeoplesearch.com\" to each URL\n",
    "final_url['Link'] = \"https://www.truepeoplesearch.com\" + final_url['Link']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(final_url)\n",
    "\n",
    "final_url.to_csv(\"./final_urls_ready.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73118985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame from the provided CSV file\n",
    "url_df = pd.read_csv(\"./final_urls_ready.csv\")\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "titles = []\n",
    "ages = []\n",
    "addresses = []  # Corrected list name\n",
    "cities = []  # Add new list for city\n",
    "states = []  # Add new list for state\n",
    "zip_codes = []  # Add new list for zip code\n",
    "home_specs = []  # Add new list for home specs\n",
    "residential_areas = []  # Add new list for residential area\n",
    "living_time_periods = []  # Add new list for living time period\n",
    "phone_1=[]\n",
    "phone_1_type=[]\n",
    "phone_2=[]\n",
    "phone_2_type=[]\n",
    "phone_3=[]\n",
    "phone_3_type=[]\n",
    "\n",
    "# Iterate over the URLs in the DataFrame\n",
    "for url in url_df['Link']:\n",
    "    # Make a request using ZenRowsClient\n",
    "    response = client.get(url, params=params)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    elements = soup.find_all(class_=\"content-container container-fluid row pl-0 pr-0\")\n",
    "    if len(elements) >= 2:\n",
    "        element2 = elements[1]\n",
    "        element_depth = element2.find(class_=\"content-center\")\n",
    "        element_more_depth = element_depth.find(class_=\"card card-body shadow-form pt-2\")\n",
    "        element_more_more_depth = element_more_depth.find(class_=\"row pl-md-2\")\n",
    "        element_more_more_more_depth = element_more_more_depth.find(class_=\"col\")\n",
    "\n",
    "        title = element_more_more_more_depth.find(\"h1\").get_text(strip=True) if element_more_more_more_depth else \"Not found\"\n",
    "        age_text_list = [span.get_text(strip=True) for span in element_more_more_more_depth.find_all(\"span\")] if element_more_more_more_depth else []\n",
    "    else:\n",
    "        title = \"Not found\"\n",
    "        age_text_list = []\n",
    "\n",
    "    address_element = soup.find('span', itemprop='streetAddress')\n",
    "\n",
    "    # Get the text of the element (address) or set to \"Not found\" if element is not found\n",
    "    address = address_element.get_text(strip=True) if address_element else \"Not found\"\n",
    "    # Additional elements for city, state, zip code, home specs, area, living time period, and phone numbers\n",
    "    city_element = soup.find('span', itemprop='addressLocality')\n",
    "    city = city_element.get_text(strip=True) if city_element else \"Not found\"\n",
    "    cities.append(city)\n",
    "\n",
    "    state_element = soup.find('span', itemprop='addressRegion')\n",
    "    state = state_element.get_text(strip=True) if state_element else \"Not found\"\n",
    "    states.append(state)\n",
    "\n",
    "    zip_element = soup.find('span', itemprop='postalCode')\n",
    "    zip_code = zip_element.get_text(strip=True) if zip_element else \"Not found\"\n",
    "    zip_codes.append(zip_code)\n",
    "    \n",
    "    home_specs_element = soup.find('span', class_=\"dt-sb\")\n",
    "    home_specs_value = home_specs_element.get_text(strip=True) if home_specs_element else \"Not found\"\n",
    "    home_specs.append(home_specs_value)\n",
    "\n",
    "    area_elements = soup.find_all('span', class_=\"dt-sb\")\n",
    "    residential_area = area_elements[1].get_text(strip=True) if len(area_elements) >= 2 else \"Not found\"\n",
    "    residential_areas.append(residential_area)\n",
    "\n",
    "    living_time_period = area_elements[2].get_text(strip=True) if len(area_elements) >= 3 else \"Not found\"\n",
    "    living_time_periods.append(living_time_period)\n",
    "\n",
    "    phone1_element = soup.find('span', itemprop='telephone')\n",
    "    phone1 = phone1_element.get_text(strip=True) if phone1_element else \"Not found\"\n",
    "    phone_1.append(phone1)\n",
    "\n",
    "    phone_type_element = soup.find('span', class_=\"smaller\")\n",
    "    phone_type = phone_type_element.get_text(strip=True) if phone_type_element else \"Not found\"\n",
    "    phone_1_type.append(phone_type)\n",
    "\n",
    "    phone2_element = soup.find_all('span', itemprop='telephone')\n",
    "    phone2 = phone2_element[1].get_text(strip=True) if len(phone2_element) >= 2 else \"Not found\"\n",
    "    phone_2.append(phone2)\n",
    "\n",
    "    phone2_type_element = soup.find_all('span', class_='smaller')\n",
    "    phone2_type = phone2_type_element[1].get_text(strip=True) if len(phone2_type_element) >= 2 else \"Not found\"\n",
    "    phone_2_type.append(phone2_type)\n",
    "\n",
    "    phone3_element = soup.find_all('span', itemprop='telephone')\n",
    "    phone3 = phone3_element[2].get_text(strip=True) if len(phone3_element) >= 3 else \"Not found\"\n",
    "    phone_3.append(phone3)\n",
    "\n",
    "    phone3_type_element = soup.find_all('span', class_='smaller')\n",
    "    phone3_type = phone3_type_element[2].get_text(strip=True) if len(phone3_type_element) >= 3 else \"Not found\"\n",
    "    phone_3_type.append(phone3_type)\n",
    "    # Append the data to the lists\n",
    "    addresses.append(address)  # Corrected list name\n",
    "    titles.append(title)\n",
    "    ages.append(\", \".join(age_text_list))\n",
    "\n",
    "# Create the DataFrame\n",
    "df_finalized = pd.DataFrame({\"Name\": titles, \"Age\": ages, \"Address\": addresses,\"City\": cities,\n",
    "    \"State\": states,\n",
    "    \"Zip Code\": zip_codes,\"Home Specs\": home_specs,\n",
    "    \"Residential Area\": residential_areas,\n",
    "    \"Living Time Period\": living_time_periods,\n",
    "                  \"Phone1\":phone_1,\n",
    "                  \"Phone1 Type\":phone_1_type,\n",
    "                  \"Phone2\":phone_2,\n",
    "                  \"Phone2 Type\":phone_2_type,\n",
    "                  \"Phone3\":phone_3,\n",
    "                  \"Phone3 Type\":phone_3_type,})  # Added \"Address\" column\n",
    "\n",
    "# Print the DataFrame\n",
    "\n",
    "df_finalized.to_excel(\"./Ready data file.xlsx\")\n",
    "\n",
    "df_finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
